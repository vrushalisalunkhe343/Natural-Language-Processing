{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a64567-01e8-4509-8cb7-39ac69ef3050",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "* Before model building, Text preprocessing steps is mandatory.\n",
    "\n",
    "*  Text preprocessing steps:\n",
    "\n",
    " 1. Sentence Tokenization\n",
    "\n",
    "2. Text cleaning with loop\n",
    "\n",
    "3. Word Tokenization\n",
    "\n",
    "4. Stop words removal\n",
    "\n",
    "5. Stemming/ Lemmatization\n",
    "\n",
    "6. Feature ExtractionÂ Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ac592e8-cac7-4e07-abda-107be7630fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarvadnya\\.jupyter\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#regural': Expected package name at the start of dependency specifier\n",
      "    #regural\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install re #regural expression USE FOR CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29450cc6-c9ff-4093-bd31-f939a6d2bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer # PorterStemmer,WordNetLemmatizer these are class.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb2eff9-aaf9-494b-860d-79d5d409135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "para='''Natural Language Processing (NLP) is a transformative field of Artificial Intelligence that focuses on\n",
    "enabling computers to understand, interpret, and generate human language in a way that is both\n",
    "meaningful and useful. From virtual assistants like Siri and Alexa to sophisticated chatbots and\n",
    "recommendation systems , NLP is now an integral part of everyday technology. It helps businesses\n",
    "analyze customer feedback , monitor social media trends , detect emotions ðŸ˜ƒðŸ˜¢ðŸ˜¡, and even\n",
    "identify sarcasm in textual data. In healthcare , NLP assists in extracting insights from medical records\n",
    ", transcribing doctor-patient conversations , flagging potential health risks , and supporting\n",
    "early disease detection . Educational platforms leverage NLP to provide personalized learning\n",
    "experiences , while e-commerce platforms offer product recommendations tailored to user\n",
    "preferences. Search engines use NLP to understand queries and deliver relevant results, whereas\n",
    "translation tools bridge language gaps and connect people across the globe . With the rise of\n",
    "generative AI models , NLP can now write articles, summarize documents, answer complex questions,\n",
    "\n",
    "and even create poetry . As NLP technology continues to advance , it is shaping the future of human-\n",
    "computer interaction, making digital communication more intelligent, accessible, and responsive than ever\n",
    "\n",
    "before.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e536794-8e28-47e1-9fb1-e4510699c566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a transformative field of Artificial Intelligence that focuses on\\nenabling computers to understand, interpret, and generate human language in a way that is both\\nmeaningful and useful. From virtual assistants like Siri and Alexa to sophisticated chatbots and\\nrecommendation systems , NLP is now an integral part of everyday technology. It helps businesses\\nanalyze customer feedback , monitor social media trends , detect emotions ðŸ˜ƒðŸ˜¢ðŸ˜¡, and even\\nidentify sarcasm in textual data. In healthcare , NLP assists in extracting insights from medical records\\n, transcribing doctor-patient conversations , flagging potential health risks , and supporting\\nearly disease detection . Educational platforms leverage NLP to provide personalized learning\\nexperiences , while e-commerce platforms offer product recommendations tailored to user\\npreferences. Search engines use NLP to understand queries and deliver relevant results, whereas\\ntranslation tools bridge language gaps and connect people across the globe . With the rise of\\ngenerative AI models , NLP can now write articles, summarize documents, answer complex questions,\\n\\nand even create poetry . As NLP technology continues to advance , it is shaping the future of human-\\ncomputer interaction, making digital communication more intelligent, accessible, and responsive than ever\\n\\nbefore.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c76dbb27-80fc-45e9-9fd1-3d4aa2b3af98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e187dad5-6019-4cb9-b5fe-c9dbdbcca963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db16ec-e29e-47f8-81fd-da6d04346106",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "036528ae-245d-4081-abf7-151ca66a93d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Sarvadnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') # RUN DUE TO PACKAGE RELATED ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84978c20-0495-47df-bc0e-ac3feabefea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) is a transformative field of Artificial Intelligence that focuses on\\nenabling computers to understand, interpret, and generate human language in a way that is both\\nmeaningful and useful.',\n",
       " 'From virtual assistants like Siri and Alexa to sophisticated chatbots and\\nrecommendation systems , NLP is now an integral part of everyday technology.',\n",
       " 'It helps businesses\\nanalyze customer feedback , monitor social media trends , detect emotions ðŸ˜ƒðŸ˜¢ðŸ˜¡, and even\\nidentify sarcasm in textual data.',\n",
       " 'In healthcare , NLP assists in extracting insights from medical records\\n, transcribing doctor-patient conversations , flagging potential health risks , and supporting\\nearly disease detection .',\n",
       " 'Educational platforms leverage NLP to provide personalized learning\\nexperiences , while e-commerce platforms offer product recommendations tailored to user\\npreferences.',\n",
       " 'Search engines use NLP to understand queries and deliver relevant results, whereas\\ntranslation tools bridge language gaps and connect people across the globe .',\n",
       " 'With the rise of\\ngenerative AI models , NLP can now write articles, summarize documents, answer complex questions,\\n\\nand even create poetry .',\n",
       " 'As NLP technology continues to advance , it is shaping the future of human-\\ncomputer interaction, making digital communication more intelligent, accessible, and responsive than ever\\n\\nbefore.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=sent_tokenize(para)\n",
    "sentences # converting paragrap into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d104aad-000c-4390-8a5e-40363aad9dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4098262c-af36-4df9-86cd-20606a4cb157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a transformative field of Artificial Intelligence that focuses on\\nenabling computers to understand, interpret, and generate human language in a way that is both\\nmeaningful and useful.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8689d28-0064-4865-a6f7-9235180d54a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d899c49d-1eb9-4139-95d2-e1b5c26167be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is nlp class'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='this is nlp class'\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261083cd-e296-427d-8681-91b699e29612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'nlp', 'class']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.split()  # word tokenization with split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec730a68-6955-4caf-95f6-be0546403a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'nlp', 'class']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b77f664c-befd-4924-9ab8-be7dc61b17cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "sentences.split() #split and word_tokenization function can  only accept string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09a6c2c9-fdd1-45ac-81e3-6cb48601bed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_tokenize(sentences)\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1328\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1327\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1457\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1458\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1429\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1428\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1431\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\.jupyter\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1394\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1392\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1393\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1397\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96e0baf2-534d-44db-a920-6f594b7000c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc8fd1-48ec-4cca-abc3-dcf9a55a28ee",
   "metadata": {},
   "source": [
    "# TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8afef039-7386-4fb1-84f6-d425e8c8ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt=[]\n",
    "for i in range(0,8): #range is use to loop inside sentence and to preform indexing\n",
    "    txt=re.sub('[^a-zA-Z]',' ',sentences[i]) # replacing all characters with space except letters.\n",
    "    txt=txt.lower() # text normalization BCAZ PYTHON IS CASE SENSETIVE\n",
    "    clean_txt.append(txt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b31cbd27-17b8-43cc-be2b-4dca0705956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing  nlp  is a transformative field of artificial intelligence that focuses on enabling computers to understand  interpret  and generate human language in a way that is both meaningful and useful ',\n",
       " 'from virtual assistants like siri and alexa to sophisticated chatbots and recommendation systems   nlp is now an integral part of everyday technology ',\n",
       " 'it helps businesses analyze customer feedback   monitor social media trends   detect emotions      and even identify sarcasm in textual data ',\n",
       " 'in healthcare   nlp assists in extracting insights from medical records   transcribing doctor patient conversations   flagging potential health risks   and supporting early disease detection  ',\n",
       " 'educational platforms leverage nlp to provide personalized learning experiences   while e commerce platforms offer product recommendations tailored to user preferences ',\n",
       " 'search engines use nlp to understand queries and deliver relevant results  whereas translation tools bridge language gaps and connect people across the globe  ',\n",
       " 'with the rise of generative ai models   nlp can now write articles  summarize documents  answer complex questions   and even create poetry  ',\n",
       " 'as nlp technology continues to advance   it is shaping the future of human  computer interaction  making digital communication more intelligent  accessible  and responsive than ever  before ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0ea3a-58d9-46e1-88cd-518496889a8b",
   "metadata": {},
   "source": [
    "# STOPWORDS REMOVAL AND STEMMING/LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "086e947d-5ca7-4564-86c0-3b95ebc9daa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing  nlp  is a transformative field of artificial intelligence that focuses on enabling computers to understand  interpret  and generate human language in a way that is both meaningful and useful ',\n",
       " 'from virtual assistants like siri and alexa to sophisticated chatbots and recommendation systems   nlp is now an integral part of everyday technology ',\n",
       " 'it helps businesses analyze customer feedback   monitor social media trends   detect emotions      and even identify sarcasm in textual data ',\n",
       " 'in healthcare   nlp assists in extracting insights from medical records   transcribing doctor patient conversations   flagging potential health risks   and supporting early disease detection  ',\n",
       " 'educational platforms leverage nlp to provide personalized learning experiences   while e commerce platforms offer product recommendations tailored to user preferences ',\n",
       " 'search engines use nlp to understand queries and deliver relevant results  whereas translation tools bridge language gaps and connect people across the globe  ',\n",
       " 'with the rise of generative ai models   nlp can now write articles  summarize documents  answer complex questions   and even create poetry  ',\n",
       " 'as nlp technology continues to advance   it is shaping the future of human  computer interaction  making digital communication more intelligent  accessible  and responsive than ever  before ']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f955831d-14dd-433d-b55b-1b4b8936dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sarvadnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78d937a3-90ae-4d49-9899-fca9ffd76250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fcaa6f8-e5e5-434f-89b7-7207bde1f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sarvadnya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5ed0dec-da28-407e-a1d9-217a087787b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()\n",
    "lemma=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "687daa5a-0213-47a7-891b-b7510befb697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chang'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ac5a8db-61d5-488c-95bf-cef3ecbb0553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001cafb2-1f48-4146-b16f-cf16efbf5761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing  nlp  is a transformative field of artificial intelligence that focuses on enabling computers to understand  interpret  and generate human language in a way that is both meaningful and useful '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8731f32f-4784-4478-a5fd-31fcc041e723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natur\n",
      "languag\n",
      "process\n",
      "nlp\n",
      "transform\n",
      "field\n",
      "artifici\n",
      "intellig\n",
      "focus\n",
      "enabl\n",
      "comput\n",
      "understand\n",
      "interpret\n",
      "gener\n",
      "human\n",
      "languag\n",
      "way\n",
      "meaning\n",
      "use\n",
      "virtual\n",
      "assist\n",
      "like\n",
      "siri\n",
      "alexa\n",
      "sophist\n",
      "chatbot\n",
      "recommend\n",
      "system\n",
      "nlp\n",
      "integr\n",
      "part\n",
      "everyday\n",
      "technolog\n",
      "help\n",
      "busi\n",
      "analyz\n",
      "custom\n",
      "feedback\n",
      "monitor\n",
      "social\n",
      "media\n",
      "trend\n",
      "detect\n",
      "emot\n",
      "even\n",
      "identifi\n",
      "sarcasm\n",
      "textual\n",
      "data\n",
      "healthcar\n",
      "nlp\n",
      "assist\n",
      "extract\n",
      "insight\n",
      "medic\n",
      "record\n",
      "transcrib\n",
      "doctor\n",
      "patient\n",
      "convers\n",
      "flag\n",
      "potenti\n",
      "health\n",
      "risk\n",
      "support\n",
      "earli\n",
      "diseas\n",
      "detect\n",
      "educ\n",
      "platform\n",
      "leverag\n",
      "nlp\n",
      "provid\n",
      "person\n",
      "learn\n",
      "experi\n",
      "e\n",
      "commerc\n",
      "platform\n",
      "offer\n",
      "product\n",
      "recommend\n",
      "tailor\n",
      "user\n",
      "prefer\n",
      "search\n",
      "engin\n",
      "use\n",
      "nlp\n",
      "understand\n",
      "queri\n",
      "deliv\n",
      "relev\n",
      "result\n",
      "wherea\n",
      "translat\n",
      "tool\n",
      "bridg\n",
      "languag\n",
      "gap\n",
      "connect\n",
      "peopl\n",
      "across\n",
      "globe\n",
      "rise\n",
      "gener\n",
      "ai\n",
      "model\n",
      "nlp\n",
      "write\n",
      "articl\n",
      "summar\n",
      "document\n",
      "answer\n",
      "complex\n",
      "question\n",
      "even\n",
      "creat\n",
      "poetri\n",
      "nlp\n",
      "technolog\n",
      "continu\n",
      "advanc\n",
      "shape\n",
      "futur\n",
      "human\n",
      "comput\n",
      "interact\n",
      "make\n",
      "digit\n",
      "commun\n",
      "intellig\n",
      "access\n",
      "respons\n",
      "ever\n"
     ]
    }
   ],
   "source": [
    "for i in  clean_txt:# loop inside list of sentence\n",
    "    words=word_tokenize(i)# each senetence will be  convert into words\n",
    "    for j in words:# looping inside list of words\n",
    "        if j not in stopwords.words('english'): # removing stopwords\n",
    "            print(stemming.stem(j)) # stemming only on non stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46e5d156-0577-49ec-9c97-103698d9a09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural\n",
      "language\n",
      "processing\n",
      "nlp\n",
      "transformative\n",
      "field\n",
      "artificial\n",
      "intelligence\n",
      "focus\n",
      "enabling\n",
      "computer\n",
      "understand\n",
      "interpret\n",
      "generate\n",
      "human\n",
      "language\n",
      "way\n",
      "meaningful\n",
      "useful\n",
      "virtual\n",
      "assistant\n",
      "like\n",
      "siri\n",
      "alexa\n",
      "sophisticated\n",
      "chatbots\n",
      "recommendation\n",
      "system\n",
      "nlp\n",
      "integral\n",
      "part\n",
      "everyday\n",
      "technology\n",
      "help\n",
      "business\n",
      "analyze\n",
      "customer\n",
      "feedback\n",
      "monitor\n",
      "social\n",
      "medium\n",
      "trend\n",
      "detect\n",
      "emotion\n",
      "even\n",
      "identify\n",
      "sarcasm\n",
      "textual\n",
      "data\n",
      "healthcare\n",
      "nlp\n",
      "assist\n",
      "extracting\n",
      "insight\n",
      "medical\n",
      "record\n",
      "transcribing\n",
      "doctor\n",
      "patient\n",
      "conversation\n",
      "flagging\n",
      "potential\n",
      "health\n",
      "risk\n",
      "supporting\n",
      "early\n",
      "disease\n",
      "detection\n",
      "educational\n",
      "platform\n",
      "leverage\n",
      "nlp\n",
      "provide\n",
      "personalized\n",
      "learning\n",
      "experience\n",
      "e\n",
      "commerce\n",
      "platform\n",
      "offer\n",
      "product\n",
      "recommendation\n",
      "tailored\n",
      "user\n",
      "preference\n",
      "search\n",
      "engine\n",
      "use\n",
      "nlp\n",
      "understand\n",
      "query\n",
      "deliver\n",
      "relevant\n",
      "result\n",
      "whereas\n",
      "translation\n",
      "tool\n",
      "bridge\n",
      "language\n",
      "gap\n",
      "connect\n",
      "people\n",
      "across\n",
      "globe\n",
      "rise\n",
      "generative\n",
      "ai\n",
      "model\n",
      "nlp\n",
      "write\n",
      "article\n",
      "summarize\n",
      "document\n",
      "answer\n",
      "complex\n",
      "question\n",
      "even\n",
      "create\n",
      "poetry\n",
      "nlp\n",
      "technology\n",
      "continues\n",
      "advance\n",
      "shaping\n",
      "future\n",
      "human\n",
      "computer\n",
      "interaction\n",
      "making\n",
      "digital\n",
      "communication\n",
      "intelligent\n",
      "accessible\n",
      "responsive\n",
      "ever\n"
     ]
    }
   ],
   "source": [
    "for i in  clean_txt:# loop inside list of sentence\n",
    "    words=word_tokenize(i)# each senetence will be  convert into words\n",
    "    for j in words:# looping inside list of words\n",
    "        if j not in stopwords.words('english'): # removing stopwords\n",
    "            print(lemma.lemmatize(j)) # stemming only on non stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729dc62-2413-4114-9b4a-3e54b95a48f6",
   "metadata": {},
   "source": [
    "# claning and saving the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1093729-6bd4-427c-870c-bba00ce49703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing  nlp  is a transformative field of artificial intelligence that focuses on enabling computers to understand  interpret  and generate human language in a way that is both meaningful and useful '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59483285-a6fb-4080-808b-f44bb16dfcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'transformative', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'useful'] "
     ]
    }
   ],
   "source": [
    "print(word_tokenize(clean_txt[0]),end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ca42671-eaf3-4e0e-b536-fc3e51bea8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=[] # create list for storing final cleaned sentences.\n",
    "for i in clean_txt: # looping inside list on sentence\n",
    "    words=word_tokenize(i)# words tokenize\n",
    "    clean_words=[] # create list for storing lemmatized words.\n",
    "    for j in words: # looping in list of words\n",
    "        if j not in stopwords.words('english'):# removing stopwords\n",
    "            clean_words.append(lemma.lemmatize(j))# preeform lemmitization and store those words in clean_words list for sentence create\n",
    "    clean_sent=' '.join(clean_words)# join lemmitized words to make sentence\n",
    "    clean.append(clean_sent) # store those sentence in different list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b60d2f8d-4400-41e5-805d-c29a97697208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp transformative field artificial intelligence focus enabling computer understand interpret generate human language way meaningful useful',\n",
       " 'virtual assistant like siri alexa sophisticated chatbots recommendation system nlp integral part everyday technology',\n",
       " 'help business analyze customer feedback monitor social medium trend detect emotion even identify sarcasm textual data',\n",
       " 'healthcare nlp assist extracting insight medical record transcribing doctor patient conversation flagging potential health risk supporting early disease detection',\n",
       " 'educational platform leverage nlp provide personalized learning experience e commerce platform offer product recommendation tailored user preference',\n",
       " 'search engine use nlp understand query deliver relevant result whereas translation tool bridge language gap connect people across globe',\n",
       " 'rise generative ai model nlp write article summarize document answer complex question even create poetry',\n",
       " 'nlp technology continues advance shaping future human computer interaction making digital communication intelligent accessible responsive ever']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b37d7-f853-40b6-9fd4-42269be6e81c",
   "metadata": {},
   "source": [
    "# feature extraction tech in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46f1a-e8ff-4741-bccf-e3ea4ea42dfe",
   "metadata": {},
   "source": [
    "* corpus-->paragraph\n",
    "* documents--> sentences\n",
    "* vocabulary-->unique words from corpus(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37cc74-6a6a-4bc3-8aba-f12121e880f3",
   "metadata": {},
   "source": [
    "* 1. binary weight---> TABULER format ,0,1, unique word in --column ,sentences in---rows\n",
    "* 2. Bag of words [BOW]----> count the number of how many time present that words in  sentence\n",
    "* 3. TF-IDF----> terms frequency-inverse document frequency\n",
    "\n",
    "\n",
    "\n",
    "*   (TF-IDF TELLS most important words in corpus) HIGH VALUE of word that word will be most important(close to 1).\n",
    "\n",
    "*     less important ( close to 0).\n",
    "\n",
    "*     high important (close  to 1).\n",
    "\n",
    "*     zero important ---> TF-IDF (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec9e5f-1c9f-4270-97bd-b3af6366fa62",
   "metadata": {},
   "source": [
    "# Binary weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99d85b63-3fc2-48a7-a767-96eb74c8d799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp transformative field artificial intelligence focus enabling computer understand interpret generate human language way meaningful useful',\n",
       " 'virtual assistant like siri alexa sophisticated chatbots recommendation system nlp integral part everyday technology',\n",
       " 'help business analyze customer feedback monitor social medium trend detect emotion even identify sarcasm textual data',\n",
       " 'healthcare nlp assist extracting insight medical record transcribing doctor patient conversation flagging potential health risk supporting early disease detection',\n",
       " 'educational platform leverage nlp provide personalized learning experience e commerce platform offer product recommendation tailored user preference',\n",
       " 'search engine use nlp understand query deliver relevant result whereas translation tool bridge language gap connect people across globe',\n",
       " 'rise generative ai model nlp write article summarize document answer complex question even create poetry',\n",
       " 'nlp technology continues advance shaping future human computer interaction making digital communication intelligent accessible responsive ever']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "534ae8a7-7114-451a-9a2d-796a136fa4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e3f81dc-9eeb-4577-a53c-95b465b733cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7354e9bf-c54a-45c5-bd0d-5136882cb094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(binary=True) # binary =True implements binary wieghts method\n",
    "x=cv.fit_transform(clean)\n",
    "x=x.toarray() # cv does not directly display the output .converting the sparse matrix into array\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d75b005-c2ea-4a14-af68-9795dad76f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['accessible', 'across', 'advance', 'ai', 'alexa', 'analyze',\n",
       "       'answer', 'article', 'artificial', 'assist', 'assistant', 'bridge',\n",
       "       'business', 'chatbots', 'commerce', 'communication', 'complex',\n",
       "       'computer', 'connect', 'continues', 'conversation', 'create',\n",
       "       'customer', 'data', 'deliver', 'detect', 'detection', 'digital',\n",
       "       'disease', 'doctor', 'document', 'early', 'educational', 'emotion',\n",
       "       'enabling', 'engine', 'even', 'ever', 'everyday', 'experience',\n",
       "       'extracting', 'feedback', 'field', 'flagging', 'focus', 'future',\n",
       "       'gap', 'generate', 'generative', 'globe', 'health', 'healthcare',\n",
       "       'help', 'human', 'identify', 'insight', 'integral', 'intelligence',\n",
       "       'intelligent', 'interaction', 'interpret', 'language', 'learning',\n",
       "       'leverage', 'like', 'making', 'meaningful', 'medical', 'medium',\n",
       "       'model', 'monitor', 'natural', 'nlp', 'offer', 'part', 'patient',\n",
       "       'people', 'personalized', 'platform', 'poetry', 'potential',\n",
       "       'preference', 'processing', 'product', 'provide', 'query',\n",
       "       'question', 'recommendation', 'record', 'relevant', 'responsive',\n",
       "       'result', 'rise', 'risk', 'sarcasm', 'search', 'shaping', 'siri',\n",
       "       'social', 'sophisticated', 'summarize', 'supporting', 'system',\n",
       "       'tailored', 'technology', 'textual', 'tool', 'transcribing',\n",
       "       'transformative', 'translation', 'trend', 'understand', 'use',\n",
       "       'useful', 'user', 'virtual', 'way', 'whereas', 'write'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out() # give all unique words from corpous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcba5f58-719f-44aa-94ee-72aaab3b2ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 71,\n",
       " 'language': 61,\n",
       " 'processing': 82,\n",
       " 'nlp': 72,\n",
       " 'transformative': 108,\n",
       " 'field': 42,\n",
       " 'artificial': 8,\n",
       " 'intelligence': 57,\n",
       " 'focus': 44,\n",
       " 'enabling': 34,\n",
       " 'computer': 17,\n",
       " 'understand': 111,\n",
       " 'interpret': 60,\n",
       " 'generate': 47,\n",
       " 'human': 53,\n",
       " 'way': 116,\n",
       " 'meaningful': 66,\n",
       " 'useful': 113,\n",
       " 'virtual': 115,\n",
       " 'assistant': 10,\n",
       " 'like': 64,\n",
       " 'siri': 97,\n",
       " 'alexa': 4,\n",
       " 'sophisticated': 99,\n",
       " 'chatbots': 13,\n",
       " 'recommendation': 87,\n",
       " 'system': 102,\n",
       " 'integral': 56,\n",
       " 'part': 74,\n",
       " 'everyday': 38,\n",
       " 'technology': 104,\n",
       " 'help': 52,\n",
       " 'business': 12,\n",
       " 'analyze': 5,\n",
       " 'customer': 22,\n",
       " 'feedback': 41,\n",
       " 'monitor': 70,\n",
       " 'social': 98,\n",
       " 'medium': 68,\n",
       " 'trend': 110,\n",
       " 'detect': 25,\n",
       " 'emotion': 33,\n",
       " 'even': 36,\n",
       " 'identify': 54,\n",
       " 'sarcasm': 94,\n",
       " 'textual': 105,\n",
       " 'data': 23,\n",
       " 'healthcare': 51,\n",
       " 'assist': 9,\n",
       " 'extracting': 40,\n",
       " 'insight': 55,\n",
       " 'medical': 67,\n",
       " 'record': 88,\n",
       " 'transcribing': 107,\n",
       " 'doctor': 29,\n",
       " 'patient': 75,\n",
       " 'conversation': 20,\n",
       " 'flagging': 43,\n",
       " 'potential': 80,\n",
       " 'health': 50,\n",
       " 'risk': 93,\n",
       " 'supporting': 101,\n",
       " 'early': 31,\n",
       " 'disease': 28,\n",
       " 'detection': 26,\n",
       " 'educational': 32,\n",
       " 'platform': 78,\n",
       " 'leverage': 63,\n",
       " 'provide': 84,\n",
       " 'personalized': 77,\n",
       " 'learning': 62,\n",
       " 'experience': 39,\n",
       " 'commerce': 14,\n",
       " 'offer': 73,\n",
       " 'product': 83,\n",
       " 'tailored': 103,\n",
       " 'user': 114,\n",
       " 'preference': 81,\n",
       " 'search': 95,\n",
       " 'engine': 35,\n",
       " 'use': 112,\n",
       " 'query': 85,\n",
       " 'deliver': 24,\n",
       " 'relevant': 89,\n",
       " 'result': 91,\n",
       " 'whereas': 117,\n",
       " 'translation': 109,\n",
       " 'tool': 106,\n",
       " 'bridge': 11,\n",
       " 'gap': 46,\n",
       " 'connect': 18,\n",
       " 'people': 76,\n",
       " 'across': 1,\n",
       " 'globe': 49,\n",
       " 'rise': 92,\n",
       " 'generative': 48,\n",
       " 'ai': 3,\n",
       " 'model': 69,\n",
       " 'write': 118,\n",
       " 'article': 7,\n",
       " 'summarize': 100,\n",
       " 'document': 30,\n",
       " 'answer': 6,\n",
       " 'complex': 16,\n",
       " 'question': 86,\n",
       " 'create': 21,\n",
       " 'poetry': 79,\n",
       " 'continues': 19,\n",
       " 'advance': 2,\n",
       " 'shaping': 96,\n",
       " 'future': 45,\n",
       " 'interaction': 59,\n",
       " 'making': 65,\n",
       " 'digital': 27,\n",
       " 'communication': 15,\n",
       " 'intelligent': 58,\n",
       " 'accessible': 0,\n",
       " 'responsive': 90,\n",
       " 'ever': 37}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e03e0be-4257-4428-9b72-1e87fe3f0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10e7ede9-ae5d-4b3c-b003-9139254087dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessible</th>\n",
       "      <th>across</th>\n",
       "      <th>advance</th>\n",
       "      <th>ai</th>\n",
       "      <th>alexa</th>\n",
       "      <th>analyze</th>\n",
       "      <th>answer</th>\n",
       "      <th>article</th>\n",
       "      <th>artificial</th>\n",
       "      <th>assist</th>\n",
       "      <th>...</th>\n",
       "      <th>translation</th>\n",
       "      <th>trend</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>useful</th>\n",
       "      <th>user</th>\n",
       "      <th>virtual</th>\n",
       "      <th>way</th>\n",
       "      <th>whereas</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accessible  across  advance  ai  alexa  analyze  answer  article  \\\n",
       "0           0       0        0   0      0        0       0        0   \n",
       "1           0       0        0   0      1        0       0        0   \n",
       "2           0       0        0   0      0        1       0        0   \n",
       "3           0       0        0   0      0        0       0        0   \n",
       "4           0       0        0   0      0        0       0        0   \n",
       "5           0       1        0   0      0        0       0        0   \n",
       "6           0       0        0   1      0        0       1        1   \n",
       "7           1       0        1   0      0        0       0        0   \n",
       "\n",
       "   artificial  assist  ...  translation  trend  understand  use  useful  user  \\\n",
       "0           1       0  ...            0      0           1    0       1     0   \n",
       "1           0       0  ...            0      0           0    0       0     0   \n",
       "2           0       0  ...            0      1           0    0       0     0   \n",
       "3           0       1  ...            0      0           0    0       0     0   \n",
       "4           0       0  ...            0      0           0    0       0     1   \n",
       "5           0       0  ...            1      0           1    1       0     0   \n",
       "6           0       0  ...            0      0           0    0       0     0   \n",
       "7           0       0  ...            0      0           0    0       0     0   \n",
       "\n",
       "   virtual  way  whereas  write  \n",
       "0        0    1        0      0  \n",
       "1        1    0        0      0  \n",
       "2        0    0        0      0  \n",
       "3        0    0        0      0  \n",
       "4        0    0        0      0  \n",
       "5        0    0        1      0  \n",
       "6        0    0        0      1  \n",
       "7        0    0        0      0  \n",
       "\n",
       "[8 rows x 119 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x,columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6e77fef-c959-4431-acb8-20c6f49b86d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing nlp transformative field artificial intelligence focus enabling computer understand interpret generate human language way meaningful useful'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856d24a-284a-4dd8-80cd-8345e73af6fb",
   "metadata": {},
   "source": [
    "# Bag of words [BOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74344d82-b1d5-4330-b6b8-4a193ba8eaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessible</th>\n",
       "      <th>across</th>\n",
       "      <th>advance</th>\n",
       "      <th>ai</th>\n",
       "      <th>alexa</th>\n",
       "      <th>analyze</th>\n",
       "      <th>answer</th>\n",
       "      <th>article</th>\n",
       "      <th>artificial</th>\n",
       "      <th>assist</th>\n",
       "      <th>...</th>\n",
       "      <th>translation</th>\n",
       "      <th>trend</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>useful</th>\n",
       "      <th>user</th>\n",
       "      <th>virtual</th>\n",
       "      <th>way</th>\n",
       "      <th>whereas</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accessible  across  advance  ai  alexa  analyze  answer  article  \\\n",
       "0           0       0        0   0      0        0       0        0   \n",
       "1           0       0        0   0      1        0       0        0   \n",
       "2           0       0        0   0      0        1       0        0   \n",
       "3           0       0        0   0      0        0       0        0   \n",
       "4           0       0        0   0      0        0       0        0   \n",
       "5           0       1        0   0      0        0       0        0   \n",
       "6           0       0        0   1      0        0       1        1   \n",
       "7           1       0        1   0      0        0       0        0   \n",
       "\n",
       "   artificial  assist  ...  translation  trend  understand  use  useful  user  \\\n",
       "0           1       0  ...            0      0           1    0       1     0   \n",
       "1           0       0  ...            0      0           0    0       0     0   \n",
       "2           0       0  ...            0      1           0    0       0     0   \n",
       "3           0       1  ...            0      0           0    0       0     0   \n",
       "4           0       0  ...            0      0           0    0       0     1   \n",
       "5           0       0  ...            1      0           1    1       0     0   \n",
       "6           0       0  ...            0      0           0    0       0     0   \n",
       "7           0       0  ...            0      0           0    0       0     0   \n",
       "\n",
       "   virtual  way  whereas  write  \n",
       "0        0    1        0      0  \n",
       "1        1    0        0      0  \n",
       "2        0    0        0      0  \n",
       "3        0    0        0      0  \n",
       "4        0    0        0      0  \n",
       "5        0    0        1      0  \n",
       "6        0    0        0      1  \n",
       "7        0    0        0      0  \n",
       "\n",
       "[8 rows x 119 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(binary=False) # binary =Flase  will use bag of words methods for table creation\n",
    "x=cv.fit_transform(clean)\n",
    "x=x.toarray()\n",
    "pd.DataFrame(x,columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9760c-eb3b-4366-9d3f-00338bd8daf1",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75818c5f-dc2d-4608-853f-e6bfc6b2f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "x=tf.fit_transform(clean)\n",
    "x=x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08713330-6e4e-4623-83ba-7a85ee97ebb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessible</th>\n",
       "      <th>across</th>\n",
       "      <th>advance</th>\n",
       "      <th>ai</th>\n",
       "      <th>alexa</th>\n",
       "      <th>analyze</th>\n",
       "      <th>answer</th>\n",
       "      <th>article</th>\n",
       "      <th>artificial</th>\n",
       "      <th>assist</th>\n",
       "      <th>...</th>\n",
       "      <th>translation</th>\n",
       "      <th>trend</th>\n",
       "      <th>understand</th>\n",
       "      <th>use</th>\n",
       "      <th>useful</th>\n",
       "      <th>user</th>\n",
       "      <th>virtual</th>\n",
       "      <th>way</th>\n",
       "      <th>whereas</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.281673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.24324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199746</td>\n",
       "      <td>0.238339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238339</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268205</td>\n",
       "      <td>0.268205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.264384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accessible    across   advance        ai     alexa   analyze    answer  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.281673  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.252358  0.000000   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5    0.000000  0.238339  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6    0.000000  0.000000  0.000000  0.268205  0.000000  0.000000  0.268205   \n",
       "7    0.264384  0.000000  0.264384  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    article  artificial    assist  ...  translation     trend  understand  \\\n",
       "0  0.000000    0.234947  0.000000  ...     0.000000  0.000000    0.196904   \n",
       "1  0.000000    0.000000  0.000000  ...     0.000000  0.000000    0.000000   \n",
       "2  0.000000    0.000000  0.000000  ...     0.000000  0.252358    0.000000   \n",
       "3  0.000000    0.000000  0.234408  ...     0.000000  0.000000    0.000000   \n",
       "4  0.000000    0.000000  0.000000  ...     0.000000  0.000000    0.000000   \n",
       "5  0.000000    0.000000  0.000000  ...     0.238339  0.000000    0.199746   \n",
       "6  0.268205    0.000000  0.000000  ...     0.000000  0.000000    0.000000   \n",
       "7  0.000000    0.000000  0.000000  ...     0.000000  0.000000    0.000000   \n",
       "\n",
       "        use    useful     user   virtual       way   whereas     write  \n",
       "0  0.000000  0.234947  0.00000  0.000000  0.234947  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.00000  0.281673  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.24324  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.238339  0.000000  0.00000  0.000000  0.000000  0.238339  0.000000  \n",
       "6  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.268205  \n",
       "7  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 119 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x,columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4248123e-b399-442d-a8c7-40970d7d45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding -->its is no represent silimilar words close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2785c13-1221-4c41-bc3c-10b4b58ef362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\Sarvadnya\\.jupyter\\anaconda3\\python.exe C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\\vendored-meson\\meson\\meson.py setup C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\\.mesonpy-odf8e2i7 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\\.mesonpy-odf8e2i7\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\n",
      "  Build dir: C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\\.mesonpy-odf8e2i7\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\Sarvadnya\\AppData\\Local\\Temp\\pip-install-59be1k7g\\numpy_adeb97fdb249416887df737654f15e6e\\.mesonpy-odf8e2i7\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07f60002-9080-4030-8a7a-a47ad7ce6b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9cbbc-63b2-40b5-8e8d-29adc1456088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1.small text data (tiny corpus)\n",
    "sentences=[\n",
    "    ['king','queen','prince','princess'],\n",
    "    ['man','woman','boy','girl']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84d694-7c6b-4778-81ce-c5199a3014fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa2c80-cf0f-4ace-ae05-da7e96959bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a34ecf-fe10-47bd-ad8c-dba9f2d0bc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9d9c2-0a6e-4d56-9f2a-49e1fae2095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84578ffa-39da-4b8f-bf3e-3eedfd6d9e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328ba3e-2219-43d3-ab24-8a095502d593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26041799-bf31-469f-85d7-367a96db98b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45b817-d3a5-49c8-8e12-9c88b4ed2c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce3778-6fac-4f81-a347-6915861ae033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce0f51-110d-49f9-94ca-2c7d20749592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90c9b-ba8d-4a8f-a9e0-d2056b3b179d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b38d1-2bd6-467a-93e4-e44796fccf05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e9d54-07a6-46f0-9f2a-8c8374a6f610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b9ce2-c276-4b04-90cc-c6b2e32c94f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f8e92-afca-4524-80fa-bf096a6df861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
